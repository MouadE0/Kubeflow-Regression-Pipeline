{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/prasannamaddila/.local/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/prasannamaddila/.local/lib/python3.11/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/prasannamaddila/.local/lib/python3.11/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/prasannamaddila/.local/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/prasannamaddila/.local/lib/python3.11/site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess() -> dict:\n",
    "   \n",
    "    # prefix = '/mnt/d/CS-KUBEFLOW'\n",
    "    prefix = 'gs://data-cs-kubeflow'\n",
    "    Path(f\"./G4\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_data = pd.read_parquet(f\"./all_data.parquet\")\n",
    "    ## Input/Outputs in the parquet files:  \n",
    "    # GR is the natural radioactivity of the rock\n",
    "    # •RHOB is the density of the rock\n",
    "    # •DTC is the compressionnal slowness through the rock\n",
    "    # •NEUT is the neutron porosity and is related to the porosity of the rock\n",
    "    # •PE is the photoelectric factor\n",
    "    # •DS_INDEX is the depth where the data were measured\n",
    "    # •DS_REF_ID is the name of the well\n",
    "    # The output is :\n",
    "    # •DT_SHEAR is Shear Slowness\n",
    "    ## \n",
    "    input_col =  [\"GR\",\"RHOB\",\"DTC\",\"NEUT\",'PE','DS_INDEX','ds_ref_id']\n",
    "    output_col = \"DT_SHEAR\"\n",
    "\n",
    "    # Subsetting the columns\n",
    "    print(\"Subsetting the columns\")\n",
    "    all_data = all_data[input_col + [output_col]]\n",
    "\n",
    "    # Create your own pipeline that will pre-process the data (explore,clean,normalize\n",
    "    # the data,feature engineering,.. )\n",
    "\n",
    "    # # Clean the data\n",
    "    print(\"Drop rows where target is NaN\")\n",
    "    all_data.dropna(subset=output_col, inplace=True)\n",
    "\n",
    "    # Split the data into train and test\n",
    "    print(\"Split the data into train and test\")\n",
    "    train_data = all_data.sample(frac=0.8,random_state=0)\n",
    "    test_data = all_data.drop(train_data.index)\n",
    "\n",
    "    # print first 5 rows of train_data\n",
    "    # Look for the cell that contains L04_06 and print the row\n",
    "    print(train_data[train_data['ds_ref_id'] == 'L04_06'])\n",
    "\n",
    "    ## METHOD 1:\n",
    "    # fill missing values with mean column values\n",
    "    # train_data.fillna(train_data.mean(), inplace=True)\n",
    "    # test_data.fillna(test_data.mean(), inplace=True)\n",
    "\n",
    "    # ## METHOD 2:\n",
    "    # # Impute other columns using KNNImputer\n",
    "    # # Create an instance of the KNNImputer class with k = 3\n",
    "\n",
    "    print(\"Impute other columns using KNNImputer\")\n",
    "    imputer = KNNImputer(n_neighbors=3)\n",
    "    # Impute missing values\n",
    "    # take into account that one of the columns ds_ref_id is a string\n",
    "    # so we need to convert it to a number\n",
    "    # we will use the label encoder\n",
    "    # create the Labelencoder object\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    # convert the categorical columns into numeric\n",
    "    train_data['ds_ref_id'] = le.fit_transform(train_data['ds_ref_id'])\n",
    "    test_data['ds_ref_id'] = le.fit_transform(test_data['ds_ref_id'])\n",
    "    # convert the dataframes to numpy arrays\n",
    "    train_data = train_data.to_numpy()\n",
    "    test_data = test_data.to_numpy()\n",
    "    # impute the missing values\n",
    "    train_data = imputer.fit_transform(train_data)\n",
    "    test_data = imputer.fit_transform(test_data)\n",
    "    \n",
    "    \n",
    "    # Convert numpy arrays back to dataframes\n",
    "    train_data = pd.DataFrame(train_data, columns=input_col + [output_col])\n",
    "    test_data = pd.DataFrame(test_data, columns=input_col + [output_col])\n",
    "    # Normalize the data  \n",
    "    # train_data = (train_data - train_data.mean()) / train_data.std()\n",
    "    # test_data = (test_data - test_data.mean()) / test_data.std()\n",
    "    print(\"Normalize the data\")\n",
    "    train_data = preprocessing.normalize(train_data)\n",
    "    test_data = preprocessing.normalize(test_data)\n",
    "\n",
    "\n",
    "    # -------- Debug ----------\n",
    "    # Count the total number of rows\n",
    "    total_rows = len(train_data)\n",
    "    print(\"total_rows: \", total_rows)\n",
    "    # Count the number of missing values per column\n",
    "    missing_values = train_data.isnull().sum()\n",
    "    print(\"missing_values: \", missing_values)\n",
    "\n",
    "    # Save the results to a text file\n",
    "    with open('{prefix}/G4/debug_train.txt', 'w') as f:\n",
    "        f.write('Total number of rows: {}\\n\\n'.format(total_rows))\n",
    "        f.write('Number of rows with missing values per column:\\n')\n",
    "        for col, count in missing_values.items():\n",
    "            if count > 0:\n",
    "                f.write('{}: {}\\n'.format(col, count))\n",
    "    #------------------------\n",
    "\n",
    "    # Save the train and test data into parquet files\n",
    "    print(\"Save the train and test data into parquet files\")\n",
    "    train_data.to_parquet(f\"{prefix}/G4/train_data.parquet\")\n",
    "    test_data.to_parquet(f\"{prefix}/G4/test_data.parquet\")\n",
    "    print(\"train_data.parquet and test_data.parquet saved in G4 folder\")\n",
    "    # return json string of train and test data addresses\n",
    "    print(\"return json string of train and test data addresses\")\n",
    "    clean_data = json.dumps({\n",
    "        'train_data': f\"{prefix}/G4/train_data.parquet\",\n",
    "        'test_data': f\"{prefix}/G4/test_data.parquet\"\n",
    "    })\n",
    "    return clean_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting the columns\n",
      "Drop rows where target is NaN\n",
      "Split the data into train and test\n",
      "                 GR     RHOB         DTC     NEUT  PE      DS_INDEX ds_ref_id   \n",
      "6013870   48.610691      NaN   76.310257      NaN NaN   8382.943359    L04_06  \\\n",
      "6003370   22.376289      NaN   81.948196      NaN NaN   4937.893066    L04_06   \n",
      "5995768         NaN      NaN  149.035553      NaN NaN   2443.677002    L04_06   \n",
      "6019590   29.182110      NaN   66.286377      NaN NaN  10259.674805    L04_06   \n",
      "6014001   38.757870      NaN   73.614014      NaN NaN   8425.923828    L04_06   \n",
      "...             ...      ...         ...      ...  ..           ...       ...   \n",
      "6022752  122.880577  2.64892   73.994202  0.19834 NaN  11297.126953    L04_06   \n",
      "6013931   46.167110      NaN   79.991791      NaN NaN   8402.957031    L04_06   \n",
      "6011729   29.268539      NaN   84.985474      NaN NaN   7680.480957    L04_06   \n",
      "6011292   29.561569      NaN   70.002472      NaN NaN   7537.101562    L04_06   \n",
      "6000667   37.897282      NaN  153.311066      NaN NaN   4051.038818    L04_06   \n",
      "\n",
      "           DT_SHEAR  \n",
      "6013870   71.183296  \n",
      "6003370   81.930199  \n",
      "5995768  155.854843  \n",
      "6019590   66.890251  \n",
      "6014001   68.349899  \n",
      "...             ...  \n",
      "6022752   74.803802  \n",
      "6013931   74.331001  \n",
      "6011729   84.764999  \n",
      "6011292   71.117104  \n",
      "6000667  153.871796  \n",
      "\n",
      "[30144 rows x 8 columns]\n",
      "Impute other columns using KNNImputer\n"
     ]
    }
   ],
   "source": [
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop rows where target is NaN\n",
      "Split the data into train and test\n",
      "Save the train and test data into parquet files\n",
      "train_data.parquet and test_data.parquet saved in G5 folder\n",
      "return json string of train and test data addresses\n"
     ]
    }
   ],
   "source": [
    "clean_data = preprocess_null(args=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count the number of missing values per column\n",
      "missing_values:  GR           0\n",
      "RHOB         0\n",
      "DENC         0\n",
      "DTC          0\n",
      "DT_SHEAR     0\n",
      "PE           0\n",
      "BS           0\n",
      "TH           0\n",
      "NEUT         0\n",
      "URAN         0\n",
      "CGR          0\n",
      "POTA         0\n",
      "CALI         0\n",
      "DEEPRES      0\n",
      "SHALRES      0\n",
      "DS_INDEX     0\n",
      "ds_ref_id    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#read the data\n",
    "prefix = '.'\n",
    "train_data = pa.read_parquet(f\"{prefix}/G5/train_data.parquet\")\n",
    "\n",
    "# count nan values\n",
    "print(\"Count the number of missing values per column\")\n",
    "missing_values = train_data.isnull().sum()\n",
    "print(\"missing_values: \", missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Score of Linear Regression is -5.14279096641701e-07\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlinear_regression\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_regression\u001b[39;00m \u001b[39mimport\u001b[39;00m linear_regression\n\u001b[1;32m      4\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m linear_regression(clean_data\u001b[39m=\u001b[39;49mclean_data, score\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m)\n",
      "File \u001b[0;32m/run/media/prasannamaddila/D/kube_workstation/asd/linear_regression/linear_regression.py:36\u001b[0m, in \u001b[0;36mlinear_regression\u001b[0;34m(clean_data, score)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe Score of Linear Regression is \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Save output into file\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(args\u001b[39m.\u001b[39mscore, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m score_file:\n\u001b[1;32m     37\u001b[0m     score_file\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(score))\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# import rf model\n",
    "from linear_regression.linear_regression import linear_regression\n",
    "\n",
    "# train the model\n",
    "model = linear_regression(clean_data=clean_data, score=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasannamaddila/.local/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.894e+09, tolerance: 3.949e+05\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Score of Lasso Regression is 0.04037727687645043\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlasso_regression\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlasso_regression\u001b[39;00m \u001b[39mimport\u001b[39;00m lasso_regression \u001b[39mas\u001b[39;00m model\n\u001b[1;32m      4\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m score \u001b[39m=\u001b[39m model(clean_data\u001b[39m=\u001b[39;49mclean_data, score\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m)\n",
      "File \u001b[0;32m/run/media/prasannamaddila/D/kube_workstation/asd/lasso_regression/lasso_regression.py:39\u001b[0m, in \u001b[0;36mlasso_regression\u001b[0;34m(clean_data, score)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe Score of Lasso Regression is \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[39m# Save output into file\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(args\u001b[39m.\u001b[39mscore, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m score_file:\n\u001b[1;32m     40\u001b[0m     score_file\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(score))\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# import rf model\n",
    "from lasso_regression.lasso_regression import lasso_regression as model\n",
    "\n",
    "# train the model\n",
    "score = model(clean_data=clean_data, score=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
